{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UG_Final_Project_Wang.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfJbpv9sCrMr"
      },
      "source": [
        "# Undergraduate Final Project: Semi-Supervised Learning\n",
        "Data Science and Applied Machine Learning <br/>\n",
        "Dr. Leslie Kerby \\\\\n",
        "Student's name: Ziming Wang\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LFQpJfECy9"
      },
      "source": [
        "**Question 1** <br/>\n",
        "Import the MNIST Digits Dataset as shown in class. Put all 70k images in one container (no splitting the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIxFPjUhAyfF"
      },
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "# not needed in Colab\n",
        "\n",
        "# Download MNIST digits\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784',version=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abRYdWGHFNCF"
      },
      "source": [
        "X, y = mnist['data'], mnist['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-Sco6RCnRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f486ba91-90a3-4bd6-b533-1fcb68e746a1"
      },
      "source": [
        "mnist['data'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnQxuz5uFxMZ"
      },
      "source": [
        "**Question 2: Semi-Supervised Learning**<br/>\n",
        "While labelling all 70k images would be very time-consuming, labelling a small subset is not. A supervised model can be trained on that small subset. This is called \"semi-supervised learning\".<br/>\n",
        "Take 150 random images from the 70k image dataset and use their labels to train a Logistic Regression model. Test it on the full dataset and report the accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aabJBEDpDf5W",
        "outputId": "0e280240-d0db-400c-9d6e-e7deedabac6a"
      },
      "source": [
        "# X is not a pandas DataFrame so we can't use sample method\n",
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# take just 150 images\n",
        "idx = np.random.permutation(70000)\n",
        "idx = idx[:150]\n",
        "X_small = X[idx]\n",
        "\n",
        "semi_log = LogisticRegression()\n",
        "semi_log.fit(X_small, y[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 230 ms, sys: 110 ms, total: 340 ms\n",
            "Wall time: 187 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jNTlr_t9ZIJ",
        "outputId": "95ab4ae9-a6df-4494-f388-5dbdf1ed33b7"
      },
      "source": [
        "print(f'Logistic Regression Semi-Supervised -- Training score: {semi_log.score(X_small,y[idx])}      Testing score: {semi_log.score(X,y)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Semi-Supervised -- Training score: 1.0      Testing score: 0.7596714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPp9E2eUcKT_"
      },
      "source": [
        "**Question 3: More Semi-Supervised Learning**<br/>\n",
        "A more sophisticated semi-supervised method is to use k-means to find \"important\" or \"representative\" instances, and then label those.<br/>\n",
        "Use k-means with k=50. Find the nearest instance to each of the 50 centroids, and these will be your 50 \"representative\" instances. Use their labels to train a Logistic Regression model. Report the accuracy.\n",
        "\n",
        "*Hint: You can use KMeans.transform() or KMeans.fit_transform() to transform X to cluster-distance space.*<br/>\n",
        "*Hint 2: Then you can use np.argmin() to find the indices of the minimum cluster-distance of each cluster. Use that as a Boolean mask on X to select out your 50 'representative' instances.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibRw1bWGQaC",
        "outputId": "b37ec3d9-ba8b-42eb-c380-4d590af721f3"
      },
      "source": [
        "# Use Mnist to find the K-means\n",
        "%%time\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k = 50\n",
        "kmeans = KMeans(n_clusters = k)\n",
        "kmeans.fit(X)\n",
        "\n",
        "dist = kmeans.transform(X)\n",
        "\n",
        "cluster_pts = np.argmin(dist, axis=0) # axis=0 for col, axis=1 for row\n",
        "# np.argmin returns the indices of the minimum values along an axis\n",
        "# cluster_pts # is the indices of nearest instance to each centroid\n",
        "\n",
        "# Use Logistic Regression model for K-means labels\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "semi_log = LogisticRegression()\n",
        "# semi_log.fit(X_small, y[idx])\n",
        "semi_log.fit(X[cluster_pts], y[cluster_pts])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 38s, sys: 19.6 s, total: 6min 57s\n",
            "Wall time: 5min 43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMse_tpilaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a71f56-0125-4386-a050-5bdc87b9d409"
      },
      "source": [
        "print(f'Logistic Regression Semi-Supervised  w/ k-means -- Training score: {semi_log.score(X[cluster_pts],y[cluster_pts])}      Testing score: {semi_log.score(X,y)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Semi-Supervised  w/ k-means -- Training score: 1.0      Testing score: 0.7757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvuUdOMcI7h6"
      },
      "source": [
        "**Question 4: Yet More Semi-Supervised Learning** \\\\\n",
        "The model from Question 3 could be improved further by assigning the 50 centroid labels to 10 instances nearby, instead of just to the nearest point. Do this and re-train your Logistic Regression model on the 500 labeled points. What is the new accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orONMbAxx5R3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaccc56-c76f-4e56-83c6-c86985d08496"
      },
      "source": [
        "%%time\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k = 50\n",
        "kmeans = KMeans(n_clusters = k)\n",
        "kmeans.fit(X)\n",
        "\n",
        "dist = kmeans.transform(X)\n",
        "\n",
        "indices = np.argpartition(dist, 10, 0)\n",
        "repr_500 = indices[:10].flatten()\n",
        "X_500 = X[repr_500]\n",
        "y_500 = y[repr_500]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "semi_log = LogisticRegression()\n",
        "semi_log.fit(X_500, y_500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7min 39s, sys: 19.8 s, total: 7min 59s\n",
            "Wall time: 6min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPxeJCfB9R5w",
        "outputId": "6312d2e7-6a7c-4aaa-fbc5-8c66da58a749"
      },
      "source": [
        "print(f'Logistic Regression Semi-Supervised  w/ k-means -- 500 labeled points nearby -- Training score: {semi_log.score(X_500, y_500)}      Testing score: {semi_log.score(X,y)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Semi-Supervised  w/ k-means -- 500 labeled points nearby -- Training score: 1.0      Testing score: 0.8092714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ZOkf6sTbnJ"
      },
      "source": [
        "**Question 5**<br/>\n",
        "Comment on the accuracy of these different semi-supervised methods. Which one was the best? Worst? Best considering time or effort? How did they compare to other supervised and unsupervised models built previously? Etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE1Gg6W3qXaA"
      },
      "source": [
        "**Answer**  \n",
        "Based on the results of these different semi-supervised methods, I think the accuracy of Q4 is the best, reaching above 0.8. The accuracy of Q3 is better than that of Q2, so the accuracy of Q2 is the worst among these different semi-supervised methods.  \n",
        "\n",
        "In terms of running time, although Q4 has the best accuracy, it takes the longest time, reaching 7 minutes. Although Q2 has the worst accuracy, it only takes about 340ms to run. The running time of Q3 and Q4 is not much different, but the result is not as good as Q4.  \n",
        "\n",
        "Compared with the unsupervised model I built in homework 9. The results of the unsupervised model that only uses K-means are far less good than the semi-supervised model. But if the t-SNE method is used, the time is much faster than the semi-supervised model, and the results can maintain the same level as Q4.  \n",
        "\n",
        "As for the supervised model, I think the accuracy of the supervised model will not be lower than the accuracy of the semi-supervised model and the unsupervised model, but the supervised model cannot do what the semi-supervised and unsupervised models can do. In general, these three different models have their own characteristics, and they have their own roles in each usage.  "
      ]
    }
  ]
}